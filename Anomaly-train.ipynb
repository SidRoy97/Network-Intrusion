{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:31: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(43, 1), padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:69: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:83: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 43, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 21, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               172160    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 172,545\n",
      "Trainable params: 172,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 140272 samples, validate on 35069 samples\n",
      "Epoch 1/25\n",
      "140272/140272 [==============================] - 34s 239us/step - loss: 0.4558 - acc: 0.7482 - val_loss: 0.1593 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98426, saving model to results/cnn1/checkpoint-01.hdf5\n",
      "Epoch 2/25\n",
      "140272/140272 [==============================] - 33s 236us/step - loss: 0.4217 - acc: 0.7773 - val_loss: 0.1436 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98426 to 0.98968, saving model to results/cnn1/checkpoint-02.hdf5\n",
      "Epoch 3/25\n",
      "140272/140272 [==============================] - 32s 230us/step - loss: 0.4140 - acc: 0.7827 - val_loss: 0.1365 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98968 to 0.99062, saving model to results/cnn1/checkpoint-03.hdf5\n",
      "Epoch 4/25\n",
      "140272/140272 [==============================] - 33s 232us/step - loss: 0.4101 - acc: 0.7869 - val_loss: 0.1783 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.99062\n",
      "Epoch 5/25\n",
      "140272/140272 [==============================] - 33s 232us/step - loss: 0.4075 - acc: 0.7887 - val_loss: 0.1551 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.99062\n",
      "Epoch 6/25\n",
      "140272/140272 [==============================] - 33s 235us/step - loss: 0.4060 - acc: 0.7908 - val_loss: 0.1415 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.99062\n",
      "Epoch 7/25\n",
      "140272/140272 [==============================] - 36s 259us/step - loss: 0.4030 - acc: 0.7923 - val_loss: 0.1472 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.99062\n",
      "Epoch 8/25\n",
      "140272/140272 [==============================] - 34s 243us/step - loss: 0.4008 - acc: 0.7958 - val_loss: 0.1497 - val_acc: 0.9614\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99062\n",
      "Epoch 9/25\n",
      "140272/140272 [==============================] - 35s 246us/step - loss: 0.3986 - acc: 0.7974 - val_loss: 0.1467 - val_acc: 0.9605\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99062\n",
      "Epoch 10/25\n",
      "140272/140272 [==============================] - 34s 240us/step - loss: 0.3971 - acc: 0.7995 - val_loss: 0.1604 - val_acc: 0.9658\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99062\n",
      "Epoch 11/25\n",
      "140272/140272 [==============================] - 35s 247us/step - loss: 0.3956 - acc: 0.8017 - val_loss: 0.1288 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.99062\n",
      "Epoch 12/25\n",
      "140272/140272 [==============================] - 35s 247us/step - loss: 0.3930 - acc: 0.8043 - val_loss: 0.1528 - val_acc: 0.9589\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.99062\n",
      "Epoch 13/25\n",
      "140272/140272 [==============================] - 35s 247us/step - loss: 0.3916 - acc: 0.8077 - val_loss: 0.1370 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.99062\n",
      "Epoch 14/25\n",
      "140272/140272 [==============================] - 34s 246us/step - loss: 0.3884 - acc: 0.8105 - val_loss: 0.1394 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.99062\n",
      "Epoch 15/25\n",
      "140272/140272 [==============================] - 40s 288us/step - loss: 0.3842 - acc: 0.8152 - val_loss: 0.1245 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.99062\n",
      "Epoch 16/25\n",
      "140272/140272 [==============================] - 36s 256us/step - loss: 0.3783 - acc: 0.8187 - val_loss: 0.1256 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.99062 to 0.99170, saving model to results/cnn1/checkpoint-16.hdf5\n",
      "Epoch 17/25\n",
      "140272/140272 [==============================] - 37s 265us/step - loss: 0.3734 - acc: 0.8233 - val_loss: 0.1320 - val_acc: 0.9905\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.99170\n",
      "Epoch 18/25\n",
      "140272/140272 [==============================] - 39s 275us/step - loss: 0.3683 - acc: 0.8283 - val_loss: 0.1073 - val_acc: 0.9963\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.99170 to 0.99626, saving model to results/cnn1/checkpoint-18.hdf5\n",
      "Epoch 19/25\n",
      "140272/140272 [==============================] - 39s 282us/step - loss: 0.3616 - acc: 0.8340 - val_loss: 0.1255 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.99626\n",
      "Epoch 20/25\n",
      "140272/140272 [==============================] - 41s 292us/step - loss: 0.3572 - acc: 0.8368 - val_loss: 0.1204 - val_acc: 0.9964\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.99626 to 0.99635, saving model to results/cnn1/checkpoint-20.hdf5\n",
      "Epoch 21/25\n",
      "140272/140272 [==============================] - 57s 403us/step - loss: 0.3529 - acc: 0.8398 - val_loss: 0.1287 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.99635\n",
      "Epoch 22/25\n",
      "140272/140272 [==============================] - 44s 314us/step - loss: 0.3506 - acc: 0.8413 - val_loss: 0.1390 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.99635\n",
      "Epoch 23/25\n",
      "140272/140272 [==============================] - 44s 317us/step - loss: 0.3496 - acc: 0.8418 - val_loss: 0.1269 - val_acc: 0.9949\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.99635\n",
      "Epoch 24/25\n",
      "140272/140272 [==============================] - 47s 333us/step - loss: 0.3474 - acc: 0.8428 - val_loss: 0.1334 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.99635\n",
      "Epoch 25/25\n",
      "140272/140272 [==============================] - 49s 349us/step - loss: 0.3464 - acc: 0.8441 - val_loss: 0.1506 - val_acc: 0.9939\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.99635\n"
     ]
    }
   ],
   "source": [
    "#CNN1\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",

    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "traindata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=1,skipfooter=35069,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "testdata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=140273,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "\n",
    "\n",
    "for column in traindata.columns:\n",
    "    if traindata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        traindata[column] = le.fit_transform(traindata[column])\n",
    "\n",
    "for column in testdata.columns:\n",
    "    if testdata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        testdata[column] = le.fit_transform(testdata[column])\n",
    "\n",
    "X = traindata.iloc[:,1:44]\n",
    "Y = traindata.iloc[:,44]\n",
    "C = testdata.iloc[:,44]\n",
    "T = testdata.iloc[:,1:44]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lstm_output_size = 128\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(43, 1)))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "print(cnn.summary())\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn1/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('results/cnn1/cnntrainanalysis1.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=25,validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"results/cnn1/cnn_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:31: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(43, 1), padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:69: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:70: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:83: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140272 samples, validate on 35069 samples\n",
      "Epoch 1/25\n",
      "140272/140272 [==============================] - 66s 473us/step - loss: 0.4342 - acc: 0.7678 - val_loss: 0.1594 - val_acc: 0.9715\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.97148, saving model to results/cnn2/checkpoint-01.hdf5\n",
      "Epoch 2/25\n",
      "140272/140272 [==============================] - 59s 418us/step - loss: 0.3785 - acc: 0.8219 - val_loss: 0.1087 - val_acc: 0.9925cc: - ETA: 6s - loss: - ETA: 0s - loss: 0.3786 - acc: \n",
      "\n",
      "Epoch 00002: val_acc improved from 0.97148 to 0.99247, saving model to results/cnn2/checkpoint-02.hdf5\n",
      "Epoch 3/25\n",
      "140272/140272 [==============================] - 62s 442us/step - loss: 0.3439 - acc: 0.8428 - val_loss: 0.0990 - val_acc: 0.9925\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.99247 to 0.99250, saving model to results/cnn2/checkpoint-03.hdf5\n",
      "Epoch 4/25\n",
      "140272/140272 [==============================] - 62s 443us/step - loss: 0.3354 - acc: 0.8445 - val_loss: 0.1434 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.99250\n",
      "Epoch 5/25\n",
      "140272/140272 [==============================] - 84s 599us/step - loss: 0.3310 - acc: 0.8453 - val_loss: 0.1367 - val_acc: 0.9926 0s - loss: 0.3310 - acc: 0.8\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.99250 to 0.99264, saving model to results/cnn2/checkpoint-05.hdf5\n",
      "Epoch 6/25\n",
      "140272/140272 [==============================] - 91s 648us/step - loss: 0.3188 - acc: 0.8460 - val_loss: 0.0937 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.99264\n",
      "Epoch 7/25\n",
      "140272/140272 [==============================] - 96s 683us/step - loss: 0.3128 - acc: 0.8469 - val_loss: 0.1687 - val_acc: 0.9941\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.99264 to 0.99410, saving model to results/cnn2/checkpoint-07.hdf5\n",
      "Epoch 8/25\n",
      "140272/140272 [==============================] - 93s 666us/step - loss: 0.3076 - acc: 0.8474 - val_loss: 0.0871 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99410\n",
      "Epoch 9/25\n",
      "140272/140272 [==============================] - 92s 654us/step - loss: 0.3059 - acc: 0.8480 - val_loss: 0.1026 - val_acc: 0.9925\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99410\n",
      "Epoch 10/25\n",
      "140272/140272 [==============================] - 96s 688us/step - loss: 0.3025 - acc: 0.8496 - val_loss: 0.0739 - val_acc: 0.9926\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99410\n",
      "Epoch 11/25\n",
      "140272/140272 [==============================] - 94s 668us/step - loss: 0.3017 - acc: 0.8508 - val_loss: 0.1090 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.99410 to 0.99527, saving model to results/cnn2/checkpoint-11.hdf5\n",
      "Epoch 12/25\n",
      "140272/140272 [==============================] - 90s 641us/step - loss: 0.3011 - acc: 0.8502 - val_loss: 0.0754 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.99527\n",
      "Epoch 13/25\n",
      "140272/140272 [==============================] - 86s 615us/step - loss: 0.3008 - acc: 0.8501 - val_loss: 0.0835 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.99527\n",
      "Epoch 14/25\n",
      "140272/140272 [==============================] - 96s 687us/step - loss: 0.3005 - acc: 0.8512 - val_loss: 0.0893 - val_acc: 0.9949\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.99527\n",
      "Epoch 15/25\n",
      "140272/140272 [==============================] - 99s 704us/step - loss: 0.3029 - acc: 0.8506 - val_loss: 0.0723 - val_acc: 0.9950\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.99527\n",
      "Epoch 16/25\n",
      "140272/140272 [==============================] - 100s 710us/step - loss: 0.3015 - acc: 0.8506 - val_loss: 0.1036 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.99527\n",
      "Epoch 17/25\n",
      "140272/140272 [==============================] - 105s 749us/step - loss: 0.2990 - acc: 0.8517 - val_loss: 0.0771 - val_acc: 0.9908TA: 15s - loss: 0.2989 - - E - ETA: 2s - loss: 0.2991 - ETA: 1s - loss:\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.99527\n",
      "Epoch 18/25\n",
      "140272/140272 [==============================] - 103s 733us/step - loss: 0.2979 - acc: 0.8522 - val_loss: 0.0703 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.99527 to 0.99578, saving model to results/cnn2/checkpoint-18.hdf5\n",
      "Epoch 19/25\n",
      "140272/140272 [==============================] - 107s 761us/step - loss: 0.2961 - acc: 0.8530 - val_loss: 0.0783 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.99578\n",
      "Epoch 20/25\n",
      "140272/140272 [==============================] - 113s 804us/step - loss: 0.2960 - acc: 0.8533 - val_loss: 0.0741 - val_acc: 0.9928\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.99578\n",
      "Epoch 21/25\n",
      "140272/140272 [==============================] - 111s 793us/step - loss: 0.2940 - acc: 0.8539 - val_loss: 0.1171 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.99578\n",
      "Epoch 22/25\n",
      "140272/140272 [==============================] - 117s 837us/step - loss: 0.2941 - acc: 0.8537 - val_loss: 0.0786 - val_acc: 0.9930 0. - ETA: 9s  - ETA: 0s - loss: 0.294\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.99578\n",
      "Epoch 23/25\n",
      "140272/140272 [==============================] - 112s 799us/step - loss: 0.2941 - acc: 0.8540 - val_loss: 0.1959 - val_acc: 0.9955\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.99578\n",
      "Epoch 24/25\n",
      "140272/140272 [==============================] - 109s 775us/step - loss: 0.2921 - acc: 0.8548 - val_loss: 0.0798 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.99578\n",
      "Epoch 25/25\n",
      "140272/140272 [==============================] - 108s 768us/step - loss: 0.2907 - acc: 0.8552 - val_loss: 0.0859 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.99578\n"
     ]
    }
   ],
   "source": [
    "#CNN2\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import GRU, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "traindata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=1,skipfooter=35069,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "testdata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=140273,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "\n",
    "\n",
    "for column in traindata.columns:\n",
    "    if traindata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        traindata[column] = le.fit_transform(traindata[column])\n",
    "\n",
    "for column in testdata.columns:\n",
    "    if testdata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        testdata[column] = le.fit_transform(testdata[column])\n",
    "\n",
    "X = traindata.iloc[:,1:44]\n",
    "Y = traindata.iloc[:,44]\n",
    "C = testdata.iloc[:,44]\n",
    "T = testdata.iloc[:,1:44]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gru_output_size = 70\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(43, 1)))\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(GRU(gru_output_size))\n",
    "cnn.add(Dropout(0.1))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn2/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('results/cnn2/cnntrainanalysis2.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=25, validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"results/cnn2/cnn_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:31: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(43, 1), padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:67: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:69: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(128, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:70: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(128, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:71: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:84: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140272 samples, validate on 35069 samples\n",
      "Epoch 1/25\n",
      "140272/140272 [==============================] - 234s 2ms/step - loss: 0.4253 - acc: 0.7809 - val_loss: 0.2021 - val_acc: 0.9623\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96230, saving model to results/cnn3/checkpoint-01.hdf5\n",
      "Epoch 2/25\n",
      "140272/140272 [==============================] - 240s 2ms/step - loss: 0.3585 - acc: 0.8375 - val_loss: 0.1555 - val_acc: 0.9905\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96230 to 0.99053, saving model to results/cnn3/checkpoint-02.hdf5\n",
      "Epoch 3/25\n",
      "140272/140272 [==============================] - 240s 2ms/step - loss: 0.3406 - acc: 0.8443 - val_loss: 0.2694 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.99053 to 0.99356, saving model to results/cnn3/checkpoint-03.hdf5\n",
      "Epoch 4/25\n",
      "140272/140272 [==============================] - 244s 2ms/step - loss: 0.3101 - acc: 0.8458 - val_loss: 0.0978 - val_acc: 0.9913oss: 0.3 - ETA: 1s - loss:\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.99356\n",
      "Epoch 5/25\n",
      "140272/140272 [==============================] - 257s 2ms/step - loss: 0.3058 - acc: 0.8477 - val_loss: 0.0982 - val_acc: 0.9898058 - - ETA\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.99356\n",
      "Epoch 6/25\n",
      "140272/140272 [==============================] - 260s 2ms/step - loss: 0.3028 - acc: 0.8482 - val_loss: 0.0726 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.99356\n",
      "Epoch 7/25\n",
      "140272/140272 [==============================] - 248s 2ms/step - loss: 0.3094 - acc: 0.8488 - val_loss: 0.1430 - val_acc: 0.9909 1s\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.99356\n",
      "Epoch 8/25\n",
      "140272/140272 [==============================] - 257s 2ms/step - loss: 0.3089 - acc: 0.8490 - val_loss: 0.0963 - val_acc: 0.9921\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99356\n",
      "Epoch 9/25\n",
      "140272/140272 [==============================] - 278s 2ms/step - loss: 0.2995 - acc: 0.8497 - val_loss: 0.0794 - val_acc: 0.9932- los\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99356\n",
      "Epoch 10/25\n",
      "140272/140272 [==============================] - 303s 2ms/step - loss: 0.2951 - acc: 0.8513 - val_loss: 0.0718 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.99356 to 0.99444, saving model to results/cnn3/checkpoint-10.hdf5\n",
      "Epoch 11/25\n",
      "140272/140272 [==============================] - 286s 2ms/step - loss: 0.2942 - acc: 0.8515 - val_loss: 0.0702 - val_acc: 0.9948A: 0s - loss: 0.2943 - ac\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.99444 to 0.99475, saving model to results/cnn3/checkpoint-11.hdf5\n",
      "Epoch 12/25\n",
      "140272/140272 [==============================] - 300s 2ms/step - loss: 0.2919 - acc: 0.8522 - val_loss: 0.0825 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.99475\n",
      "Epoch 13/25\n",
      "140272/140272 [==============================] - 287s 2ms/step - loss: 0.2931 - acc: 0.8529 - val_loss: 0.0730 - val_acc: 0.9919\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.99475\n",
      "Epoch 14/25\n",
      "140272/140272 [==============================] - 217s 2ms/step - loss: 0.2904 - acc: 0.8542 - val_loss: 0.0890 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.99475\n",
      "Epoch 15/25\n",
      "140272/140272 [==============================] - 217s 2ms/step - loss: 0.2891 - acc: 0.8545 - val_loss: 0.0786 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.99475\n",
      "Epoch 16/25\n",
      "140272/140272 [==============================] - 222s 2ms/step - loss: 0.2865 - acc: 0.8571 - val_loss: 0.0839 - val_acc: 0.9881\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.99475\n",
      "Epoch 17/25\n",
      "140272/140272 [==============================] - 226s 2ms/step - loss: 0.2840 - acc: 0.8591 - val_loss: 0.0702 - val_acc: 0.9946\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.99475\n",
      "Epoch 18/25\n",
      "140272/140272 [==============================] - 266s 2ms/step - loss: 0.2841 - acc: 0.8595 - val_loss: 0.0725 - val_acc: 0.9930\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.99475\n",
      "Epoch 19/25\n",
      "140272/140272 [==============================] - 331s 2ms/step - loss: 0.2830 - acc: 0.8607 - val_loss: 0.0701 - val_acc: 0.9941\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.99475\n",
      "Epoch 20/25\n",
      "140272/140272 [==============================] - 354s 3ms/step - loss: 0.2819 - acc: 0.8616 - val_loss: 0.0659 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.99475 to 0.99478, saving model to results/cnn3/checkpoint-20.hdf5\n",
      "Epoch 21/25\n",
      "140272/140272 [==============================] - 354s 3ms/step - loss: 0.2818 - acc: 0.8622 - val_loss: 0.0758 - val_acc: 0.9926\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.99478\n",
      "Epoch 22/25\n",
      "140272/140272 [==============================] - 343s 2ms/step - loss: 0.2818 - acc: 0.8612 - val_loss: 0.0705 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.99478\n",
      "Epoch 23/25\n",
      "140272/140272 [==============================] - 346s 2ms/step - loss: 0.2799 - acc: 0.8634 - val_loss: 0.0606 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.99478 to 0.99527, saving model to results/cnn3/checkpoint-23.hdf5\n",
      "Epoch 24/25\n",
      "140272/140272 [==============================] - 350s 2ms/step - loss: 0.2784 - acc: 0.8635 - val_loss: 0.0727 - val_acc: 0.9917\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.99527\n",
      "Epoch 25/25\n",
      "140272/140272 [==============================] - 348s 2ms/step - loss: 0.2795 - acc: 0.8631 - val_loss: 0.0680 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.99527\n"
     ]
    }
   ],
   "source": [
    "#CNN3\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "traindata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=1,skipfooter=35069,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "testdata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=140273,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "\n",
    "\n",
    "for column in traindata.columns:\n",
    "    if traindata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        traindata[column] = le.fit_transform(traindata[column])\n",
    "\n",
    "for column in testdata.columns:\n",
    "    if testdata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        testdata[column] = le.fit_transform(testdata[column])\n",
    "\n",
    "X = traindata.iloc[:,1:44]\n",
    "Y = traindata.iloc[:,44]\n",
    "C = testdata.iloc[:,44]\n",
    "T = testdata.iloc[:,1:44]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))\n",
    "\n",
    "\n",
    "lstm_output_size = 128\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(43, 1)))\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(Convolution1D(128, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(Convolution1D(128, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn3/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('results/cnn3/cnntrainanalysis3.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=25,validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"results/cnn3/cnn_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:31: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(43, 1), padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:78: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140272 samples, validate on 35069 samples\n",
      "Epoch 1/25\n",
      "140272/140272 [==============================] - 151s 1ms/step - loss: 0.4929 - acc: 0.7191 - val_loss: 0.1791 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85283, saving model to results/cnn-gru1results/checkpoint-01.hdf5\n",
      "Epoch 2/25\n",
      "140272/140272 [==============================] - 163s 1ms/step - loss: 0.4397 - acc: 0.7645 - val_loss: 0.1258 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85283 to 0.98845, saving model to results/cnn-gru1results/checkpoint-02.hdf5\n",
      "Epoch 3/25\n",
      "140272/140272 [==============================] - 202s 1ms/step - loss: 0.4247 - acc: 0.7841 - val_loss: 0.1303 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.98845\n",
      "Epoch 4/25\n",
      "140272/140272 [==============================] - 220s 2ms/step - loss: 0.4156 - acc: 0.7933 - val_loss: 0.1562 - val_acc: 0.9516\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.98845\n",
      "Epoch 5/25\n",
      "140272/140272 [==============================] - 262s 2ms/step - loss: 0.4118 - acc: 0.7938 - val_loss: 0.1286 - val_acc: 0.9715\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.98845\n",
      "Epoch 6/25\n",
      "140272/140272 [==============================] - 306s 2ms/step - loss: 0.4100 - acc: 0.7955 - val_loss: 0.1536 - val_acc: 0.9646\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.98845\n",
      "Epoch 7/25\n",
      "140272/140272 [==============================] - 322s 2ms/step - loss: 0.4100 - acc: 0.7962 - val_loss: 0.1459 - val_acc: 0.9578\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.98845\n",
      "Epoch 8/25\n",
      "140272/140272 [==============================] - 355s 3ms/step - loss: 0.4048 - acc: 0.7984 - val_loss: 0.1296 - val_acc: 0.9698\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.98845\n",
      "Epoch 9/25\n",
      "140272/140272 [==============================] - 380s 3ms/step - loss: 0.4017 - acc: 0.7997 - val_loss: 0.1541 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98845\n",
      "Epoch 10/25\n",
      "140272/140272 [==============================] - 404s 3ms/step - loss: 0.4000 - acc: 0.8004 - val_loss: 0.1444 - val_acc: 0.9602\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.98845\n",
      "Epoch 11/25\n",
      "140272/140272 [==============================] - 438s 3ms/step - loss: 0.4005 - acc: 0.7996 - val_loss: 0.1315 - val_acc: 0.9645ss: 0.4006 - acc: 0\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.98845\n",
      "Epoch 12/25\n",
      "140272/140272 [==============================] - 474s 3ms/step - loss: 0.3988 - acc: 0.8008 - val_loss: 0.1371 - val_acc: 0.9560\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.98845\n",
      "Epoch 13/25\n",
      "140272/140272 [==============================] - 502s 4ms/step - loss: 0.3972 - acc: 0.8019 - val_loss: 0.1573 - val_acc: 0.9522\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.98845\n",
      "Epoch 14/25\n",
      " 47392/140272 [=========>....................] - ETA: 5:43 - loss: 0.3966 - acc: 0.8023"
     ]
    }
   ],
   "source": [
    "#CNN-gru1\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import GRU, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "traindata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=1,skipfooter=35069,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "testdata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=140273,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "\n",
    "\n",
    "for column in traindata.columns:\n",
    "    if traindata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        traindata[column] = le.fit_transform(traindata[column])\n",
    "\n",
    "for column in testdata.columns:\n",
    "    if testdata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        testdata[column] = le.fit_transform(testdata[column])\n",
    "\n",
    "X = traindata.iloc[:,1:44]\n",
    "Y = traindata.iloc[:,44]\n",
    "C = testdata.iloc[:,44]\n",
    "T = testdata.iloc[:,1:44]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))\n",
    "\n",
    "gru_output_size = 70\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(43, 1)))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(GRU(gru_output_size))\n",
    "cnn.add(Dropout(0.1))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn-gru1results/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('results/cnn-gru1results/cnntrainanalysis1.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=25, validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"results/cnn-gru1results/cnn_model.hdf5\")\n",
    "\n",
    "'''\n",
    "\n",
    "cnn.load_weights(\"results/cnn-gru1results/checkpoint-947.hdf5\")\n",
    "\n",
    "\n",
    "cnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "loss, accuracy = cnn.evaluate(X_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "\n",
    "\n",
    "y_pred = cnn.predict_classes(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred , average=\"binary\")\n",
    "precision = precision_score(y_test, y_pred , average=\"binary\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "np.savetxt('res/expected1.txt', y_test, fmt='%01d')\n",
    "np.savetxt('res/predicted1.txt', y_pred, fmt='%01d')\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"accuracy\")\n",
    "print(\"%.6f\" %accuracy)\n",
    "print(\"racall\")\n",
    "print(\"%.6f\" %recall)\n",
    "print(\"precision\")\n",
    "print(\"%.6f\" %precision)\n",
    "print(\"f1score\")\n",
    "print(\"%.6f\" %f1)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"==============================================\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:31: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(43, 1), padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:67: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:79: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140272 samples, validate on 35069 samples\n",
      "Epoch 1/25\n",
      "140272/140272 [==============================] - 204s 1ms/step - loss: 0.4695 - acc: 0.7460 - val_loss: 0.1360 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.95178, saving model to results/cnn-gru2results/checkpoint-01.hdf5\n",
      "Epoch 2/25\n",
      "140272/140272 [==============================] - 284s 2ms/step - loss: 0.4243 - acc: 0.7825 - val_loss: 0.1605 - val_acc: 0.9541\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.95178 to 0.95412, saving model to results/cnn-gru2results/checkpoint-02.hdf5\n",
      "Epoch 3/25\n",
      "140272/140272 [==============================] - 432s 3ms/step - loss: 0.4170 - acc: 0.7894 - val_loss: 0.1461 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.95412\n",
      "Epoch 4/25\n",
      "140272/140272 [==============================] - 522s 4ms/step - loss: 0.4074 - acc: 0.7956 - val_loss: 0.1441 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.95412 to 0.97773, saving model to results/cnn-gru2results/checkpoint-04.hdf5\n",
      "Epoch 5/25\n",
      "140272/140272 [==============================] - 606s 4ms/step - loss: 0.4042 - acc: 0.7984 - val_loss: 0.1379 - val_acc: 0.9656\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.97773\n",
      "Epoch 6/25\n",
      "140272/140272 [==============================] - 646s 5ms/step - loss: 0.4028 - acc: 0.7988 - val_loss: 0.1427 - val_acc: 0.9591\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.97773\n",
      "Epoch 7/25\n",
      "140272/140272 [==============================] - 710s 5ms/step - loss: 0.4048 - acc: 0.7984 - val_loss: 0.1450 - val_acc: 0.9545\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.97773\n",
      "Epoch 8/25\n",
      "140272/140272 [==============================] - 805s 6ms/step - loss: 0.4043 - acc: 0.7979 - val_loss: 0.1531 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.97773\n",
      "Epoch 9/25\n",
      "140272/140272 [==============================] - 892s 6ms/step - loss: 0.4028 - acc: 0.7983 - val_loss: 0.1386 - val_acc: 0.9671\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.97773\n",
      "Epoch 10/25\n",
      "140272/140272 [==============================] - 1004s 7ms/step - loss: 0.3965 - acc: 0.8019 - val_loss: 0.1274 - val_acc: 0.9638\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.97773\n",
      "Epoch 11/25\n",
      "140272/140272 [==============================] - 1106s 8ms/step - loss: 0.3952 - acc: 0.8023 - val_loss: 0.1520 - val_acc: 0.9451\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.97773\n",
      "Epoch 12/25\n",
      "140272/140272 [==============================] - 1194s 9ms/step - loss: 0.3962 - acc: 0.8003 - val_loss: 0.1523 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.97773\n",
      "Epoch 13/25\n",
      "129408/140272 [==========================>...] - ETA: 1:35 - loss: 0.3955 - acc: 0.8029"
     ]
    }
   ],
   "source": [
    "#CNN-GRU2\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import GRU, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "traindata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=1,skipfooter=35069,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "testdata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=140273,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "\n",
    "\n",
    "for column in traindata.columns:\n",
    "    if traindata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        traindata[column] = le.fit_transform(traindata[column])\n",
    "\n",
    "for column in testdata.columns:\n",
    "    if testdata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        testdata[column] = le.fit_transform(testdata[column])\n",
    "\n",
    "X = traindata.iloc[:,1:44]\n",
    "Y = traindata.iloc[:,44]\n",
    "C = testdata.iloc[:,44]\n",
    "T = testdata.iloc[:,1:44]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))\n",
    "\n",
    "gru_output_size = 70\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(43, 1)))\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(GRU(gru_output_size))\n",
    "cnn.add(Dropout(0.1))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn-gru2results/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('results/cnn-gru2results/cnntrainanalysis2.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=25,validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"results/cnn-gru2results/cnn_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:32: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:69: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(43, 1), padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:70: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:71: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:72: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(128, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:73: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(128, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140272 samples, validate on 35069 samples\n",
      "Epoch 1/25\n",
      "140272/140272 [==============================] - 166s 1ms/step - loss: 0.4380 - acc: 0.7738 - val_loss: 0.1461 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.93031, saving model to results/cnn-gru3results/checkpoint-01.hdf5\n",
      "Epoch 2/25\n",
      "140272/140272 [==============================] - 219s 2ms/step - loss: 0.3797 - acc: 0.8185 - val_loss: 0.1139 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.93031 to 0.99481, saving model to results/cnn-gru3results/checkpoint-02.hdf5\n",
      "Epoch 3/25\n",
      "140272/140272 [==============================] - 291s 2ms/step - loss: 0.3492 - acc: 0.8426 - val_loss: 0.1185 - val_acc: 0.9925\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.99481\n",
      "Epoch 4/25\n",
      "140272/140272 [==============================] - 312s 2ms/step - loss: 0.3434 - acc: 0.8469 - val_loss: 0.1348 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.99481\n",
      "Epoch 5/25\n",
      "140272/140272 [==============================] - 345s 2ms/step - loss: 0.3394 - acc: 0.8489 - val_loss: 0.1057 - val_acc: 0.9929\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.99481\n",
      "Epoch 6/25\n",
      "140272/140272 [==============================] - 418s 3ms/step - loss: 0.3383 - acc: 0.8488 - val_loss: 0.1315 - val_acc: 0.9922\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.99481\n",
      "Epoch 7/25\n",
      "140272/140272 [==============================] - 576s 4ms/step - loss: 0.3331 - acc: 0.8518 - val_loss: 0.1153 - val_acc: 0.9939\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.99481\n",
      "Epoch 8/25\n",
      "140272/140272 [==============================] - 673s 5ms/step - loss: 0.3336 - acc: 0.8512 - val_loss: 0.1308 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99481\n",
      "Epoch 9/25\n",
      "140272/140272 [==============================] - 732s 5ms/step - loss: 0.3258 - acc: 0.8585 - val_loss: 0.1107 - val_acc: 0.9955\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.99481 to 0.99547, saving model to results/cnn-gru3results/checkpoint-09.hdf5\n",
      "Epoch 10/25\n",
      "140272/140272 [==============================] - 617s 4ms/step - loss: 0.3199 - acc: 0.8638 - val_loss: 0.1234 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99547\n",
      "Epoch 11/25\n",
      "140272/140272 [==============================] - 629s 4ms/step - loss: 0.3184 - acc: 0.8648 - val_loss: 0.1324 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.99547\n",
      "Epoch 12/25\n",
      "140272/140272 [==============================] - 696s 5ms/step - loss: 0.3156 - acc: 0.8668 - val_loss: 0.1163 - val_acc: 0.9921\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.99547\n",
      "Epoch 13/25\n",
      "140272/140272 [==============================] - 726s 5ms/step - loss: 0.3112 - acc: 0.8677 - val_loss: 0.1126 - val_acc: 0.9876\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.99547\n",
      "Epoch 14/25\n",
      "140272/140272 [==============================] - 793s 6ms/step - loss: 0.2803 - acc: 0.8704 - val_loss: 0.1091 - val_acc: 0.9932\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.99547\n",
      "Epoch 15/25\n",
      "140272/140272 [==============================] - 849s 6ms/step - loss: 0.2772 - acc: 0.8716 - val_loss: 0.0740 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.99547\n",
      "Epoch 16/25\n",
      "140272/140272 [==============================] - 901s 6ms/step - loss: 0.2750 - acc: 0.8721 - val_loss: 0.0798 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.99547\n",
      "Epoch 17/25\n",
      "140272/140272 [==============================] - 963s 7ms/step - loss: 0.2726 - acc: 0.8725 - val_loss: 0.0703 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.99547\n",
      "Epoch 18/25\n",
      "140272/140272 [==============================] - 994s 7ms/step - loss: 0.2708 - acc: 0.8737 - val_loss: 0.1039 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.99547\n",
      "Epoch 19/25\n",
      "140272/140272 [==============================] - 1068s 8ms/step - loss: 0.2704 - acc: 0.8736 - val_loss: 0.0877 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.99547\n",
      "Epoch 20/25\n",
      "140272/140272 [==============================] - 1131s 8ms/step - loss: 0.2668 - acc: 0.8750 - val_loss: 0.0595 - val_acc: 0.9932\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.99547\n",
      "Epoch 21/25\n",
      "140272/140272 [==============================] - 1162s 8ms/step - loss: 0.2666 - acc: 0.8747 - val_loss: 0.0694 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.99547\n",
      "Epoch 22/25\n",
      "140256/140272 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.8752"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-8dac88d6ed18>\", line 86, in <module>\n",
      "    cnn.fit(X_train, y_train, nb_epoch=25, validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1039, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\engine\\training_arrays.py\", line 212, in fit_loop\n",
      "    verbose=0)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\engine\\training_arrays.py\", line 392, in test_loop\n",
      "    batch_outs = f(ins_batch)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2715, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2675, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1458, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\inspect.py\", line 1488, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\inspect.py\", line 1450, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 185, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\tokenize.py\", line 452, in open\n",
      "    buffer = _builtin_open(filename, 'rb')\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#CNN-GRU3\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import GRU, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "traindata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=1,skipfooter=35069,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "testdata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=140273,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "\n",
    "\n",
    "for column in traindata.columns:\n",
    "    if traindata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        traindata[column] = le.fit_transform(traindata[column])\n",
    "\n",
    "for column in testdata.columns:\n",
    "    if testdata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        testdata[column] = le.fit_transform(testdata[column])\n",
    "\n",
    "X = traindata.iloc[:,1:44]\n",
    "Y = traindata.iloc[:,44]\n",
    "C = testdata.iloc[:,44]\n",
    "T = testdata.iloc[:,1:44]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gru_output_size = 70\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(43, 1)))\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(Convolution1D(128, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(Convolution1D(128, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(GRU(gru_output_size))\n",
    "cnn.add(Dropout(0.1))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn-gru3results/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('results/cnn-gru3results/cnntrainanalysis3.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=25, validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"results/cnn-gru3results/cnn_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:32: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(43, 1), padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:67: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:79: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140272 samples, validate on 35069 samples\n",
      "Epoch 1/25\n",
      "140272/140272 [==============================] - 197s 1ms/step - loss: 0.4848 - acc: 0.7162 - val_loss: 0.1946 - val_acc: 0.9691\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96909, saving model to results/cnn-lstm1results/checkpoint-01.hdf5\n",
      "Epoch 2/25\n",
      "140272/140272 [==============================] - 217s 2ms/step - loss: 0.4487 - acc: 0.7534 - val_loss: 0.1370 - val_acc: 0.9982\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96909 to 0.99818, saving model to results/cnn-lstm1results/checkpoint-02.hdf5\n",
      "Epoch 3/25\n",
      "140272/140272 [==============================] - 313s 2ms/step - loss: 0.4272 - acc: 0.7739 - val_loss: 0.1629 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.99818\n",
      "Epoch 4/25\n",
      "140272/140272 [==============================] - 413s 3ms/step - loss: 0.4175 - acc: 0.7833 - val_loss: 0.1482 - val_acc: 0.9867: 2s - \n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.99818\n",
      "Epoch 5/25\n",
      "140272/140272 [==============================] - 425s 3ms/step - loss: 0.4119 - acc: 0.7929 - val_loss: 0.1506 - val_acc: 0.9368 -\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.99818\n",
      "Epoch 6/25\n",
      "140272/140272 [==============================] - 485s 3ms/step - loss: 0.4070 - acc: 0.7976 - val_loss: 0.1475 - val_acc: 0.9564\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.99818\n",
      "Epoch 7/25\n",
      "140272/140272 [==============================] - 561s 4ms/step - loss: 0.4059 - acc: 0.7976 - val_loss: 0.1380 - val_acc: 0.9478\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.99818\n",
      "Epoch 8/25\n",
      "140272/140272 [==============================] - 629s 4ms/step - loss: 0.4004 - acc: 0.7998 - val_loss: 0.1512 - val_acc: 0.9593\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99818\n",
      "Epoch 9/25\n",
      "140272/140272 [==============================] - 701s 5ms/step - loss: 0.3984 - acc: 0.8010 - val_loss: 0.1417 - val_acc: 0.9463\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99818\n",
      "Epoch 10/25\n",
      "140272/140272 [==============================] - 775s 6ms/step - loss: 0.3986 - acc: 0.8017 - val_loss: 0.1452 - val_acc: 0.9952\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99818\n",
      "Epoch 11/25\n",
      "140272/140272 [==============================] - 874s 6ms/step - loss: 0.3866 - acc: 0.8110 - val_loss: 0.1357 - val_acc: 0.9875\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.99818\n",
      "Epoch 12/25\n",
      "132544/140272 [===========================>..] - ETA: 52s - loss: 0.3602 - acc: 0.8299"
     ]
    }
   ],
   "source": [
    "#CNN-LSTM1\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "traindata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=1,skipfooter=35069,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "testdata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=140273,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "\n",
    "\n",
    "for column in traindata.columns:\n",
    "    if traindata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        traindata[column] = le.fit_transform(traindata[column])\n",
    "\n",
    "for column in testdata.columns:\n",
    "    if testdata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        testdata[column] = le.fit_transform(testdata[column])\n",
    "\n",
    "X = traindata.iloc[:,1:44]\n",
    "Y = traindata.iloc[:,44]\n",
    "C = testdata.iloc[:,44]\n",
    "T = testdata.iloc[:,1:44]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(43, 1)))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(LSTM(lstm_output_size))\n",
    "cnn.add(Dropout(0.1))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn-lstm1results/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('results/cnn-lstm1results/cnntrainanalysis1.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=25,validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"results/cnn-lstm1results/cnn_model.hdf5\")\n",
    "\n",
    "'''\n",
    "\n",
    "cnn.load_weights(\"results/cnn-lstm1results/checkpoint-947.hdf5\")\n",
    "\n",
    "\n",
    "cnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "loss, accuracy = cnn.evaluate(X_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))\n",
    "\n",
    "\n",
    "y_pred = cnn.predict_classes(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred , average=\"binary\")\n",
    "precision = precision_score(y_test, y_pred , average=\"binary\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "np.savetxt('res/expected1.txt', y_test, fmt='%01d')\n",
    "np.savetxt('res/predicted1.txt', y_pred, fmt='%01d')\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"accuracy\")\n",
    "print(\"%.6f\" %accuracy)\n",
    "print(\"racall\")\n",
    "print(\"%.6f\" %recall)\n",
    "print(\"precision\")\n",
    "print(\"%.6f\" %precision)\n",
    "print(\"f1score\")\n",
    "print(\"%.6f\" %f1)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"==============================================\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:32: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(43, 1), padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:67: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:80: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140272 samples, validate on 35069 samples\n",
      "Epoch 1/25\n",
      "140272/140272 [==============================] - 192s 1ms/step - loss: 0.4805 - acc: 0.7151 - val_loss: 0.1798 - val_acc: 0.9871\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98714, saving model to results/cnn-lstm2results/checkpoint-01.hdf5\n",
      "Epoch 2/25\n",
      "140272/140272 [==============================] - 248s 2ms/step - loss: 0.4255 - acc: 0.7783 - val_loss: 0.1709 - val_acc: 0.9358\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.98714\n",
      "Epoch 3/25\n",
      "140272/140272 [==============================] - 359s 3ms/step - loss: 0.4126 - acc: 0.7884 - val_loss: 0.1413 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.98714\n",
      "Epoch 4/25\n",
      "140272/140272 [==============================] - 421s 3ms/step - loss: 0.4130 - acc: 0.7914 - val_loss: 0.1476 - val_acc: 0.9291:  - ETA: 4:39 - loss: 0.4226  - ETA: 4:38 - - ET - ETA: 4:24 - loss: 0 - ETA: 4:23 - loss: 0.4219 - acc: - ETA: 4:22 - - ETA: 4:04 - loss: 0.4206 - acc - ETA: 4:03 - loss: 0.4208 - ac - ETA: 4:02 - loss: 0.4206 - acc: 0.78 - ETA: 4:02 - loss: 0.4207 - acc: 0.78 -  - ETA: 3:52 - loss: 0.4192 - acc: 0.78 - ETA: 3:52 - loss: 0.4191 - acc: 0.7 - ETA: 3:52 - loss: 0.4190 - acc: - ETA: 3:51 - lo - ETA: 3:49 - loss: 0.4186 - acc: 0 - ETA: 3:48 - loss: 0.4185  -  - ETA: 3:34 - loss: 0.41 - ETA: 3:32 - loss: 0.4184 - acc: 0.788 - ETA: 3:32 - loss: 0.4185  - ETA: 3:31 - loss: 0.418 - ETA: 3:27 - loss: 0.4180 - acc:  - ETA: 3:26 - los - ETA: 3:24 - loss: 0.4183 - acc: 0.78 - ETA: 3:24 - loss: 0.41 - ETA: 3:23 - loss: 0.4181 - acc: 0.788 - ETA: 3:23 - loss: 0.4181 - acc:   - ETA: 3:19 - loss: - ETA: 3:17 - loss: 0.4178 - acc: 0.78 - ETA: 3:17 - loss: 0.4178 - acc: 0. - ETA: 3:16 - loss: 0.4177 - acc: 0.78 - ETA: 3:16 - loss: 0.41 - ETA: 3:15 - los - ETA: 3:13 - loss: 0.4175 - ETA: 3:12 - loss: 0.4175 - acc: 0.78 - ETA: 3:11 - loss: 0.4175 - acc: 0 - ETA: 3:11 - loss: 0.4177 - - ETA: 3:06 - loss: 0.4178 - - ETA: 3:02 - loss: 0. - ETA: 3:00 - loss: 0.4175 - acc: - ETA: 2:59 - loss: 0.4175 - acc: - ETA: 2:59 - loss: 0.4176 - acc:  - ETA: 2:58 - loss: 0.4177 - acc: 0.789 -  - ETA: - ETA: 2:52 - loss: 0.4167 - acc: 0 - ETA: 2:52 - loss: 0.4166 - a - ETA: 2:51 - loss: 0.4166 - - - ETA: 2:46 - loss: 0.4161 - acc - ETA: 2:46 - loss: 0.4160 - acc - ETA: 2:45 - lo - ETA: 2:29 - loss: 0.4151 -  - ETA: 2:28 - loss: 0.4150 - acc:  - ETA: 2:24 - loss: 0.4 - ETA: 2:23 -  - ETA: 2:20 - loss: 0.4155 - acc: 0.7 - ETA: 2:20 - loss: 0.4156 - acc: 0. - ETA: 2:20 - loss: 0.4154 - acc:  - ETA: 2:19 - loss: 0.4153 - acc: 0 - ETA: 2:19 - loss: 0.4153 - ac - ETA: 2 - ETA:  - - ETA: 2:06 - loss: 0.4150 - ac - ETA: 1:59 - loss: 0.4140 - acc: 0.790 - ETA: 1:58 - loss - ETA: 1:57 - loss: 0.4140 - - ETA: 1:45 - loss: 0.4142 - acc: 0.790 - ETA: 1:45 - loss: 0.4142 - ETA: 1:44 - loss: 0.4139 - ac - ETA: 1:43 - loss: 0.4139 - acc: 0.790 - ETA: 1:43 - loss: 0.4139 -  - ETA: 1:42 - loss: 0.4142 - acc: 0.79 - ETA - ETA: 1: - ETA: 1:21 - loss: 0.4146 - acc - ETA: 1:20 - loss: 0.4147 - acc: 0.7 - ETA: 1:20 - loss: 0. - - ETA: 1:11 - l\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.98714\n",
      "Epoch 5/25\n",
      "140272/140272 [==============================] - 493s 4ms/step - loss: 0.4126 - acc: 0.7936 - val_loss: 0.1468 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.98714\n",
      "Epoch 6/25\n",
      "140272/140272 [==============================] - 580s 4ms/step - loss: 0.4090 - acc: 0.7963 - val_loss: 0.2170 - val_acc: 0.8884\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.98714\n",
      "Epoch 7/25\n",
      "140272/140272 [==============================] - 681s 5ms/step - loss: 0.4110 - acc: 0.7906 - val_loss: 0.1705 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.98714\n",
      "Epoch 8/25\n",
      "140272/140272 [==============================] - 774s 6ms/step - loss: 0.4033 - acc: 0.7969 - val_loss: 0.1413 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.98714\n",
      "Epoch 9/25\n",
      "140272/140272 [==============================] - 851s 6ms/step - loss: 0.4010 - acc: 0.7986 - val_loss: 0.1313 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98714\n",
      "Epoch 10/25\n",
      "140272/140272 [==============================] - 992s 7ms/step - loss: 0.3999 - acc: 0.7997 - val_loss: 0.1390 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.98714\n",
      "Epoch 11/25\n",
      "140272/140272 [==============================] - 1106s 8ms/step - loss: 0.3965 - acc: 0.8019 - val_loss: 0.1577 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.98714\n",
      "Epoch 12/25\n",
      "140272/140272 [==============================] - 1209s 9ms/step - loss: 0.3976 - acc: 0.8001 - val_loss: 0.1428 - val_acc: 0.9620\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.98714\n",
      "Epoch 13/25\n",
      "138560/140272 [============================>.] - ETA: 15s - loss: 0.3953 - acc: 0.8016"
     ]
    }
   ],
   "source": [
    "#CNN-lstm2\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "traindata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=1,skipfooter=35069,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "testdata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=140273,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "\n",
    "\n",
    "for column in traindata.columns:\n",
    "    if traindata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        traindata[column] = le.fit_transform(traindata[column])\n",
    "\n",
    "for column in testdata.columns:\n",
    "    if testdata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        testdata[column] = le.fit_transform(testdata[column])\n",
    "\n",
    "X = traindata.iloc[:,1:44]\n",
    "Y = traindata.iloc[:,44]\n",
    "C = testdata.iloc[:,44]\n",
    "T = testdata.iloc[:,1:44]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(43, 1)))\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(LSTM(lstm_output_size))\n",
    "cnn.add(Dropout(0.1))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn-lstm2results/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('results/cnn-lstm2results/cnntrainanalysis2.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=25,validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"results/cnn-lstm2results/cnn_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:32: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:69: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", input_shape=(43, 1), padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:70: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:71: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:72: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(128, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:73: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(128, 3, activation=\"relu\", padding=\"same\")`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n",
      "C:\\Users\\Vamsi\\AppData\\Local\\conda\\conda\\envs\\PythonCPU\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140272 samples, validate on 35069 samples\n",
      "Epoch 1/25\n",
      "140272/140272 [==============================] - 238s 2ms/step - loss: 0.4378 - acc: 0.7751 - val_loss: 0.1651 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.94882, saving model to results/cnn-lstm3results/checkpoint-01.hdf5\n",
      "Epoch 2/25\n",
      "140272/140272 [==============================] - 323s 2ms/step - loss: 0.4090 - acc: 0.7957 - val_loss: 0.1363 - val_acc: 0.9603\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.94882 to 0.96031, saving model to results/cnn-lstm3results/checkpoint-02.hdf5\n",
      "Epoch 3/25\n",
      "140272/140272 [==============================] - 440s 3ms/step - loss: 0.3967 - acc: 0.8034 - val_loss: 0.1498 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.96031 to 0.97821, saving model to results/cnn-lstm3results/checkpoint-03.hdf5\n",
      "Epoch 4/25\n",
      "140272/140272 [==============================] - 587s 4ms/step - loss: 0.3625 - acc: 0.8335 - val_loss: 0.1499 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.97821\n",
      "Epoch 5/25\n",
      "140272/140272 [==============================] - 701s 5ms/step - loss: 0.3498 - acc: 0.8433 - val_loss: 0.1266 - val_acc: 0.9890\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.97821 to 0.98902, saving model to results/cnn-lstm3results/checkpoint-05.hdf5\n",
      "Epoch 6/25\n",
      "140272/140272 [==============================] - 838s 6ms/step - loss: 0.3437 - acc: 0.8473 - val_loss: 0.1328 - val_acc: 0.9776 9:43 - loss: 0.34 - ETA: 9:41 - loss: 0.3454 - a - ETA: 9:40 - loss - ETA: 9:37 -  - ETA: - ETA: 9:21 - - E - ETA: 9:06 - loss: 0.3431 - acc - ETA: 9: - ETA: 9:00 - loss: 0.3433 - a - ETA: 7:24 - loss: 0.3423 - - ETA:  - ETA: 7:17 - loss:  - ETA: 7:09 - loss: 0.3432 - ETA: 7:06 - loss: 0.3431 -  - ETA: 7:05 - loss: 0.3429 - acc: 0. - ETA: 7:04 - l - ETA: 7:00 - loss:  - ETA: 6: - ETA: 6:46 - loss: 0.3436 - acc: - ETA: 6:45 - loss: 0.3436 - acc: 0.84 - ETA: 6:45 - loss: 0.3435 - acc: 0.848 - ETA: 6: - ETA: 6:40  - ETA: 6:36 - loss: 0.3439 - acc:  - ETA: 6:35 - loss:  - ETA: 6:3 - ETA: 6:27 - loss: 0.3443 - acc - ETA: 6:25 - loss: 0.3443 - acc: 0.84 - ETA: 6:25 - loss: 0.3442 - acc: 0.847 - ETA: 6:25 - l - ETA: 6:21 - loss: 0.3443 -  - ETA: 6:19 - loss: 0.3444 - a - ETA: 6:18 - loss: 0.3445 - acc: 0 - ETA: 6:17 - loss: 0.3447 - acc: - ETA: 6:10 - loss:  - ETA: 6:06 - loss: 0.3439 - acc: 0.84 - ETA: 6:06 -  - ETA: 5:55 - loss: 0.3441 - acc: 0.847 - ETA: 5:55 - loss: 0.3441 - acc:  - ETA: 5:54 - loss: 0.3442 - acc - ETA: 5:53 - loss: 0.3441 - acc: 0. - ETA: 5:52 - loss: 0.3441 - acc: 0 - ETA: 5:51 - l - ETA: 5:48 - loss: 0.3442 - ac - ETA: 5:46 - loss: 0.3443 - - ETA: 5:44 - loss: 0.3443 - acc: - ETA: 5:43 - loss: 0.3446 - acc: 0.84 - ETA: 5:43 - loss: 0.3446 - acc: 0.847 - - ETA: 5:36 - loss: 0.3446 - acc: 0 - ETA: 5:29 - loss: 0.3441 - acc: 0.8 - ETA: 5:29 - loss: 0.3442 - acc:  - ETA: 5:28 - loss: 0.3442  - ETA: 5:26 - loss: 0.3444 -  - ETA: 5:24 - loss:  - ETA: 5:14 - loss: 0.3445 - acc:  - ETA: 5:14 - loss:  - ETA: 5:10  - ETA: 5:06 - loss: 0.3443 - - ETA: 5:04 - loss: 0.344 - ETA: 5:01 - loss: 0.3443 - acc: 0.847 - ETA: 5:01 - loss: - ETA: 4:58 - loss: 0.3 - ETA: 4:55 - loss: 0.3438 - ac - ETA: 4:54 - ETA: 4:50 - loss: 0.3439 - acc - ETA: 4:48 - loss: 0.3441 - acc: 0. - ETA: 4:48 - loss: 0.3441 - acc: 0.84 - ETA: 4:47 - loss: 0.3441 - - ETA: 4:46 - loss: 0.3443 - acc - ETA: 4:45 - l - ETA: 4:40 - loss: 0.3447 - acc: 0.847 - ETA: 4: - ETA: 4:35 - loss:  - ETA: 4:25 - lo - ETA: 4:21 - loss:  - ETA: 4:18 - loss: 0.344 - ETA:  - ETA: 4:10 - loss: 0.3440 - acc: 0 - ETA: 4:09 - loss: 0.3443 - acc: 0.848 - ETA: 4:09 -  - ETA: 4:05 - loss: 0.3441  - ETA: 4:03 - loss: 0.3442 - acc: 0 - ETA: 4:02  - ETA: 3:58 - loss: - ETA: 3:54 - loss: 0.3446 - ac - ETA: 3:53 - loss: 0.3445 - acc: 0.84 - ETA: 3:52 - loss: 0.3445 - acc: 0 - ETA: 3: - ETA: 3:46 - loss: 0.3445 - acc: 0 - ETA: 3:46 - loss: 0.3445 - acc:  - ETA: 3:45 - loss: 0.3445 - acc: 0.847 - ETA: 3:44 - loss: 0.3445 - acc - ETA: 3:43 - loss: 0.3445 - acc:  - ETA: 3:42 - loss: 0.3445 - acc: 0.8 - ETA: 3:42 - loss: 0.34 - ETA: 3:39 - loss: 0.3446 - acc:  - ETA: 3:38 - loss: 0.3447 - acc: 0.847 - ETA: 3:38 - loss: 0.3447 - acc: 0.84 - ETA: 3:37 - loss: 0.3447 - acc: 0.8 - ETA: 3:37 - loss: 0.3447 -  - ETA: 3:35 - loss: 0.3447 - acc: 0.847 - ETA: 3:35 - loss: 0.3447 - acc: 0. - ETA: 3:34 - loss: 0.3447 - acc: 0 - ETA: 3:33 - loss: 0.3446 - acc: 0.8 - ETA: 3:3 - ETA: 3:28 - loss: 0.3447 - acc: 0.8 - ETA: 3:27 - loss: 0.3447 - acc - ETA: 3:26 - loss: 0.3447 - acc: 0.8 - ETA: 3:26 - l - ETA: 3:21 - loss: 0.3447 - acc:  - ETA: 3:20 - loss: 0.3448  - ETA: 3:18 - loss: 0.3449 - ac - ETA: 3:17 - loss: 0.3449 - acc: 0 - ETA: 3:16 - loss: 0.3449 - acc:  - ETA: 3:15 - loss: 0.3447 - acc - ETA: 3:14 - loss: 0.3445 - acc: 0.84 - ETA: 3:13 - loss: 0.3445 - acc: 0.84 - ETA: 3:13 - loss:  - ETA: 3:10 - loss: 0.3446 - acc: 0.8 - ETA: 3:09 - loss: 0.3446 - acc - ETA: 3 - ETA: 2:56 - loss: 0.3449 - acc: 0.84 - ETA: 2:55 - loss: 0.3449 - acc: 0.847 - ETA: 2:55 - loss: 0.3449 - - ETA: 2:53 - loss: 0.3449 - acc - ETA: 2:52 - loss: 0.3449 - acc: 0.84 - ETA: 2:51 - loss: 0 - ET - ETA: 2:43 - loss: 0.3446 - ac - ET - ETA: 2:35 - loss: 0.3444 - acc: 0 - ETA: 2:34 - loss: 0.3444 - acc: 0. - ETA: 2:34 - loss: 0.3444 - acc: 0.8 - ETA: 2:33 - loss: 0.344 - ETA: 2:24 - loss: 0.3444 - acc: 0.847 - ETA: 2:23 - loss: 0.3444 - acc:  - ETA: 2:22 - loss: 0.3443 - - ETA: 2:20 - loss: 0.3444 - acc:  - ETA: 2:19 - loss: 0.3444 - acc: 0 - ETA: 2:18 - loss: 0.3444 - acc: 0 - ETA: 2:18 - loss: 0. - ETA: 2:15  - ETA: 2:10 - loss: 0.3443 - acc: 0 - ETA: 2:09 - loss: 0.3 - ETA: 2:06 - loss: 0.3443 - ETA: 2:04 - loss: 0.3444 - a - ETA: - ETA: 1:49 - loss: 0.3444 - acc: - ETA: 1:48 - loss: 0.3445 - a - ETA: 1:46 - loss: 0.3445 - acc: 0.847 - ETA: 1:46 - loss: 0.3445 - acc: 0 - ETA: 1:45 - loss: 0.3445 - ETA: 1:36 - loss: 0.3442   - - ETA: 24s - lo - E - ET - ETA: 2s - loss: 0.3437 -\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.98902\n",
      "Epoch 7/25\n",
      "140272/140272 [==============================] - 811s 6ms/step - loss: 0.3420 - acc: 0.8476 - val_loss: 0.1040 - val_acc: 0.9942\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.98902 to 0.99424, saving model to results/cnn-lstm3results/checkpoint-07.hdf5\n",
      "Epoch 8/25\n",
      "140272/140272 [==============================] - 830s 6ms/step - loss: 0.3371 - acc: 0.8499 - val_loss: 0.1269 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.99424\n",
      "Epoch 9/25\n",
      "140272/140272 [==============================] - 952s 7ms/step - loss: 0.3067 - acc: 0.8528 - val_loss: 0.0959 - val_acc: 0.9904\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.99424\n",
      "Epoch 10/25\n",
      "140272/140272 [==============================] - 1074s 8ms/step - loss: 0.2915 - acc: 0.8603 - val_loss: 0.0781 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99424\n",
      "Epoch 11/25\n",
      "140272/140272 [==============================] - 1187s 8ms/step - loss: 0.2826 - acc: 0.8665 - val_loss: 0.0694 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.99424\n",
      "Epoch 12/25\n",
      "140272/140272 [==============================] - 1309s 9ms/step - loss: 0.2768 - acc: 0.8702 - val_loss: 0.0810 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.99424 to 0.99601, saving model to results/cnn-lstm3results/checkpoint-12.hdf5\n",
      "Epoch 13/25\n",
      "140272/140272 [==============================] - 1454s 10ms/step - loss: 0.2736 - acc: 0.8724 - val_loss: 0.2907 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.99601\n",
      "Epoch 14/25\n",
      "140272/140272 [==============================] - 1580s 11ms/step - loss: 0.2959 - acc: 0.8629 - val_loss: 0.1391 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.99601\n",
      "Epoch 15/25\n",
      "140272/140272 [==============================] - 1714s 12ms/step - loss: 0.3336 - acc: 0.8545 - val_loss: 0.1230 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.99601\n",
      "Epoch 16/25\n",
      "140272/140272 [==============================] - 1859s 13ms/step - loss: 0.3331 - acc: 0.8537 - val_loss: 0.1341 - val_acc: 0.9865\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.99601\n",
      "Epoch 17/25\n",
      "140272/140272 [==============================] - 1997s 14ms/step - loss: 0.3282 - acc: 0.8565 - val_loss: 0.1378 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.99601\n",
      "Epoch 18/25\n",
      "140272/140272 [==============================] - 2073s 15ms/step - loss: 0.3118 - acc: 0.8582 - val_loss: 0.1059 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.99601\n",
      "Epoch 19/25\n",
      "140272/140272 [==============================] - 2304s 16ms/step - loss: 0.2847 - acc: 0.8652 - val_loss: 0.1226 - val_acc: 0.9889\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.99601\n",
      "Epoch 20/25\n",
      "140272/140272 [==============================] - 2360s 17ms/step - loss: 0.2808 - acc: 0.8679 - val_loss: 0.0857 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.99601\n",
      "Epoch 21/25\n",
      "140272/140272 [==============================] - 2435s 17ms/step - loss: 0.2893 - acc: 0.8628 - val_loss: 0.1310 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.99601\n",
      "Epoch 22/25\n",
      "140272/140272 [==============================] - 2844s 20ms/step - loss: 0.2842 - acc: 0.8654 - val_loss: 0.0919 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.99601\n",
      "Epoch 23/25\n",
      " 60416/140272 [===========>..................] - ETA: 29:52 - loss: 0.2783 - acc: 0.8691"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras import callbacks\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score, accuracy_score,mean_squared_error,mean_absolute_error)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "traindata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=1,skipfooter=35069,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "testdata=pd.read_csv('UNSW_NB15_training_set.csv',skiprows=140273,names=['id','dur','proto','service','state','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','dwin','tcprtt','synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd','ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports','attack_cat','label'])\n",
    "\n",
    "\n",
    "for column in traindata.columns:\n",
    "    if traindata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        traindata[column] = le.fit_transform(traindata[column])\n",
    "\n",
    "for column in testdata.columns:\n",
    "    if testdata[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        testdata[column] = le.fit_transform(testdata[column])\n",
    "\n",
    "X = traindata.iloc[:,1:44]\n",
    "Y = traindata.iloc[:,44]\n",
    "C = testdata.iloc[:,44]\n",
    "T = testdata.iloc[:,1:44]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "trainX = scaler.transform(X)\n",
    "\n",
    "scaler = Normalizer().fit(T)\n",
    "testT = scaler.transform(T)\n",
    "\n",
    "y_train = np.array(Y)\n",
    "y_test = np.array(C)\n",
    "\n",
    "X_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\n",
    "X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\",activation=\"relu\",input_shape=(43, 1)))\n",
    "cnn.add(Convolution1D(64, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(Convolution1D(128, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(Convolution1D(128, 3, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling1D(pool_length=(2)))\n",
    "cnn.add(LSTM(lstm_output_size))\n",
    "cnn.add(Dropout(0.1))\n",
    "cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "\n",
    "cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath=\"results/cnn-lstm3results/checkpoint-{epoch:02d}.hdf5\", verbose=1, save_best_only=True, monitor='val_acc',mode='max')\n",
    "csv_logger = CSVLogger('results/cnn-lstm3results/cnntrainanalysis3.csv',separator=',', append=False)\n",
    "cnn.fit(X_train, y_train, nb_epoch=25, validation_data=(X_test, y_test),callbacks=[checkpointer,csv_logger])\n",
    "cnn.save(\"results/cnn-lstm3results/cnn_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
